https://colab.research.google.com
https://colab.research.google.com/notebooks/intro.ipynb#scrollTo=UdRyKR44dcNI


>>>import nltk
>>>nltk.download('book')

>>>from nltk.book import* 
>>>text4

Определение длины текста (слова + знаки препинания)
>>>len(text4)

Словарь текста
>>>sorted (set(text3))

Число различных слов в словаре
>>>len(set(text3))

мера лексического разнообразия текста
>>>len (set (text3)) / len (text3) 

>>>text1.count ("monstrous")


>>>set(text4)

>>>len(text1)

>>>sorted(set(text2))

Поиск вхождения данного слова  в текст вместе 
с некоторым контекстом
>>>text1.concordance ("monstrous")

Поиск слов похожих по значению на указанное в контексте
>>>text1.similar ("monstrous")

Можно исследовать контексты, общие для двух или более слов
>>>text2.common_contexts (["monstrous","very"])

Можно определить  частотное расположение слов в тексте - график дисперсии
>>>text4.dispersion_plot(["citizen", "democracy","freedom", "America"])

Текст как список

>>>text1[25:30]

>>>sent1 = ['Call', 'me', 'Ishmael', '.'] 
>>>len (sent1) 

Конкатенация в списках
>>>[ 'Monty' , 'Python' ] + [ 'and' , 'the' , 'Holy' , 'Grail' ] 


>>>sent4=['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the','House', 'of', 'Representatives',':']
>>>sent4 + sent1

Добавление элемента в список
>>>sent1.append ( “Some" )
>>>sent1 

>>>Text4 [173]

'awaken'

>>>Text4.index ('awaken')
173

>>>text5 [16715: 16735] 
['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good', 'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without', 'buying', 'it']

 Порядок индексации:
>>> sent = [ 'word1' , 'word2' , 'word3' , 'word4' , 'word5' , ... 'word6' , 'word7' , 'word8' , 'word9' , 'word10' ] 
>> sent [0] 
'word1' 
>>> sent [9] 
'word10‘

Поиск по индексам
 >>> sent[5: 8] 
['word6', 'word7', 'word8']


Имя переменной не может быть каким-либо из зарезервированных слов Python, таких как def , if , not и import
>>> Dictionary = set (text1) 
>>> vocab_size = len (vocab) 
>>> vocab_size 
19317
Присвоение строки переменной
>>> name = ' 'Monty'
 Индексация строки
>>> name [0] 
'М‘
Нарезка строки
>>> name[: 4] 
'Mont‘

Умножение и сложение со строками:
>>> name * 2 
'MontyMonty' 
>>> name + '!' 
'Монти!‘

объединить слова списка в одну строку или разбить строку на список
>>> '' .join ([ 'Монти' , 'Python' ]) 'Монти Пайтон' 
>>> 'Монти Пайтон' .split () ['Монти', 'Python']


Пример
my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode', ... 'forth', 'from', 'Camelot', '.'] >>> noun_phrase = my_sent[1:4] 
>>> noun_phrase ['bold', 'Sir', 'Robin'] 
>>> wOrDs = sorted(noun_phrase) 
>>> wOrDs ['Robin', 'Sir', 'bold'] 
>>>


Частотный анализ текста

>>> fdist1 = FreqDist (текст1)
 >>> print (fdist1) 
<FreqDist с 19317 выборками и 260819 исходами> 
>>> fdist1.most_common (50) [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ( 'to', 4542), (';', 4072), ('in', 3916), ('that', 2982), ("'", 2684), (' - ', 2552), (' его ', 2459), (' оно ', 2209), (' I ', 2124), (' s ', 1739), (' is ', 1695), (' he ', 1661), (' with ', 1659), ('было', 1632), ('как', 1620), ('"', 1478), ('все', 1462), ('для', 1414), ('это', 1280) , ('!', 1269), ('at', 1231), ('by', 1137), ('но', 1113), ('not', 1103), ('-', 1070), ('он', 1058), ('от', 1052), ('быть', 1030), ('он', 1005), ('так', 918), ('кит', 906), ('один', 889), ('ты', 841), ('имел', 767), ('иметь', 760), (' там ', 715), (' Но ', 705), (' или ', 697), (' были ', 680), (' сейчас ', 646), (' который ', 640), ('? ' , 637), ('я', 627), ('как', 624)] 
>>> fdist1 [ 'кит' ] 
906

График совокупной частоты для 50 наиболее часто встречающихся слов в Моби Дике 
fdist1.plot (50, cumulative = True)

 
Частотный анализ текста с учетом длины слов и частоты
>>> fdist5 = FreqDist(text5) 
>>> sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7) 
['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question', 'actually', 'anything', 'computer', 
'cute.-ass', 'everyone', 'football', 'innocent', 'listening',
 'remember', 'seriously', 'something', 'together', 'tomorrow', 'watching'] 

распределение длин слов в тексте
>>> [len(w) for w in text1] 
[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...] 
>>> fdist = FreqDist(len(w) for w in text1) 
>>> print(fdist) 
<FreqDist with 19 samples and 260819 outcomes> 
>>> fdist 
FreqDist({3: 50223, 1: 47933, 4: 42345, 2: 38513, 5: 26597, 6: 17111, 7: 14399, 8: 9966, 9: 6428, 10: 3528, ...})
 >>>

насколько часто встречаются слова разной длины
>>> fdist.most_common() 
[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399), (8, 9966), (9, 6428), (10, 3528), 
(11, 1873), (12, 1053), (13, 567), (14, 177), (15, 70), (16, 22), (17, 12), (18, 1), (20, 1)] 
>>> fdist.max() 
3 
>>> fdist[3] 
50223 
>>> fdist.freq(3) 
0.19255882431878046 
>>> 

Токенизация

>>> import nltk 
>>> sentence = """At eight o'clock on Thursday morning 
... Arthur didn't feel very good.""" 
>>> tokens = nltk.word_tokenize(sentence) 
>>> tokens 
['At', 'eight', "o'clock", 'on', 'Thursday', 'morning', 'Arthur', 'did', "n't", 'feel', 'very', 'good', '.'] 
>>> tagged = nltk.pos_tag(tokens) 
>>> tagged[0:6] 
[('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]





